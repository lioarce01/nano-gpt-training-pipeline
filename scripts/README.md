# Scripts / Utility Tools

Herramientas auxiliares para trabajar con modelos entrenados.

## üìÅ Contenido

```
scripts/
‚îú‚îÄ‚îÄ sample.py          # Generar texto desde un modelo entrenado
‚îú‚îÄ‚îÄ merge_lora.py      # Mergear adaptadores LoRA en modelo base
‚îî‚îÄ‚îÄ test_lora.py       # Comparar modelo base vs LoRA fine-tuned
```

---

## üéØ sample.py - Generaci√≥n de Texto

Genera texto desde un modelo entrenado (GPT est√°ndar o con LoRA).

### Uso B√°sico

```bash
# Generar desde modelo en models/
python scripts/sample.py --checkpoint models/gpt-38M-scientific-pretrain/ckpt.pt

# Con prompt personalizado
python scripts/sample.py \
    --checkpoint models/gpt-38M-scientific-pretrain/ckpt.pt \
    --prompt "In this paper we present" \
    --num_samples 3 \
    --max_tokens 200
```

### Par√°metros

```
--checkpoint     Path al checkpoint (.pt) (required)
--prompt         Texto inicial (default: "\n")
--num_samples    N√∫mero de muestras a generar (default: 1)
--max_tokens     Tokens a generar por muestra (default: 100)
--temperature    Temperatura de sampling (default: 0.8)
                 Mayor = m√°s aleatorio, menor = m√°s determinista
--top_k          Top-k sampling (default: 200)
--device         Device: cpu o cuda (default: cpu)
--seed           Semilla random (default: 1337)
```

### Ejemplos

**Modelo cient√≠fico:**
```bash
python scripts/sample.py \
    --checkpoint models/gpt-38M-scientific-pretrain/ckpt.pt \
    --prompt "Abstract: " \
    --max_tokens 150 \
    --temperature 0.7
```

**Modelo LoRA fine-tuned:**
```bash
python scripts/sample.py \
    --checkpoint models/gpt-38M-tinystories-to-scientific-lora/ckpt.pt \
    --prompt "In this study, we investigate" \
    --num_samples 5
```

**Sampling creativo (alta temperatura):**
```bash
python scripts/sample.py \
    --checkpoint models/gpt-38M-scientific-pretrain/ckpt.pt \
    --temperature 1.2 \
    --top_k 50
```

---

## üîÄ merge_lora.py - Mergear LoRA

Combina adaptadores LoRA con el modelo base para crear un modelo est√°ndar (sin overhead de LoRA).

### ¬øPor qu√© mergear?

**Modelo con LoRA:**
- Checkpoint peque√±o (~5-10 MB de adaptadores)
- Requiere cargar base + adaptadores
- Ligero overhead en inferencia

**Modelo mergeado:**
- Checkpoint completo (~150 MB para 38M params)
- Carga directa, sin dependencias
- Inferencia m√°s r√°pida (sin capa extra de LoRA)

### Uso

```bash
# Mergear modelo LoRA
python scripts/merge_lora.py \
    --lora_checkpoint models/gpt-38M-tinystories-to-scientific-lora/ckpt.pt \
    --output_dir models/gpt-38M-tinystories-to-scientific-merged \
    --device cpu
```

### Par√°metros

```
--lora_checkpoint    Path al checkpoint LoRA (required)
--output_dir         Directorio de salida para modelo mergeado (required)
--device             Device: cpu o cuda (default: cpu)
```

### Flujo T√≠pico

```bash
# 1. Fine-tune con LoRA
python train.py config/finetune_lora.py

# 2. Mergear para deployment
python scripts/merge_lora.py \
    --lora_checkpoint models/gpt-38M-tinystories-to-scientific-lora/ckpt.pt \
    --output_dir models/gpt-38M-tinystories-to-scientific-merged

# 3. Usar modelo mergeado
python scripts/sample.py \
    --checkpoint models/gpt-38M-tinystories-to-scientific-merged/ckpt.pt
```

---

## üî¨ test_lora.py - Comparaci√≥n de Modelos

Compara generaci√≥n de texto entre modelo base y modelo LoRA fine-tuned.

### Uso

```bash
python scripts/test_lora.py \
    --base_checkpoint models/gpt-38M-tinystories-pretrain/ckpt.pt \
    --lora_checkpoint models/gpt-38M-tinystories-to-scientific-lora/ckpt.pt \
    --prompt "Once upon a time" \
    --max_tokens 100
```

### Par√°metros

```
--base_checkpoint    Path al modelo base (required)
--lora_checkpoint    Path al modelo LoRA (required)
--prompt             Texto inicial (default: "Once upon a time")
--max_tokens         Tokens a generar (default: 100)
--temperature        Temperatura de sampling (default: 0.8)
--device             Device: cpu o cuda (default: cpu)
```

### Ejemplo de Salida

```
======================================================================
BASE MODEL (TinyStories):
======================================================================
Once upon a time there was a little girl named Lily. She loved to play
outside in the sun. One day she saw a big red ball...

======================================================================
LORA FINE-TUNED MODEL (Scientific):
======================================================================
Once upon a time, the field of machine learning was primarily focused
on supervised learning approaches. Recent advances in self-supervised...

======================================================================
COMPARISON COMPLETE
======================================================================
```

---

## üõ†Ô∏è Notas T√©cnicas

### Path Handling

Todos los scripts en `scripts/` agregan autom√°ticamente el directorio ra√≠z al `sys.path`:

```python
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
```

Esto permite importar m√≥dulos del root (`model.py`, `checkpoint.py`, etc.) sin problemas.

### Checkpoints Compatibles

Los scripts funcionan con:
- ‚úÖ Checkpoints est√°ndar (pretraining desde scratch)
- ‚úÖ Checkpoints LoRA (PEFT)
- ‚úÖ Checkpoints mergeados
- ‚úÖ Formatos: `.pt` (PyTorch) y `.safetensors`

### Performance Tips

**Para generaci√≥n r√°pida:**
- Usa `--temperature 0.5` (m√°s determinista, menos c√≥mputo)
- Limita `--max_tokens` a lo necesario
- Usa `--device cuda` si tienes GPU

**Para generaci√≥n creativa:**
- Usa `--temperature 1.0-1.2` (m√°s variedad)
- Ajusta `--top_k` (50-200, menor = m√°s conservador)
- Genera m√∫ltiples muestras con `--num_samples`

---

## üìö Ejemplos Completos

### Pipeline Completo: Pretrain ‚Üí LoRA ‚Üí Merge ‚Üí Sample

```bash
# 1. Pretrain base model
python train.py config/train_tinystories.py

# 2. LoRA fine-tune to scientific domain
python train.py config/finetune_lora.py

# 3. Test comparison
python scripts/test_lora.py \
    --base_checkpoint models/gpt-38M-tinystories-pretrain/ckpt.pt \
    --lora_checkpoint models/gpt-38M-tinystories-to-scientific-lora/ckpt.pt

# 4. Merge for deployment
python scripts/merge_lora.py \
    --lora_checkpoint models/gpt-38M-tinystories-to-scientific-lora/ckpt.pt \
    --output_dir models/gpt-38M-tinystories-to-scientific-merged

# 5. Generate scientific text
python scripts/sample.py \
    --checkpoint models/gpt-38M-tinystories-to-scientific-merged/ckpt.pt \
    --prompt "Abstract: In this paper we" \
    --num_samples 3 \
    --max_tokens 200 \
    --temperature 0.7
```

---

## ‚ùì FAQ

**Q: ¬øPor qu√© est√°n en scripts/ y no en root?**
A: Para mantener el root limpio. Solo `train.py` y m√≥dulos core (`model.py`, `checkpoint.py`) deben estar en root.

**Q: ¬øPuedo usar estos scripts con modelos de otros proyectos?**
A: Solo si usan la misma arquitectura GPT y formato de checkpoint. Para otros modelos, necesitar√≠an adaptaci√≥n.

**Q: ¬øC√≥mo elijo la temperatura?**
A:
- 0.1-0.5: Muy determinista (buen para QA, c√≥digo)
- 0.6-0.9: Balanceado (buen default)
- 1.0-1.5: Creativo (buen para historias, ideas)

**Q: ¬øQu√© hace top_k?**
A: Limita el sampling a los K tokens m√°s probables. Menor = m√°s conservador.

---

## üîó Ver Tambi√©n

- `../TRAINING_GUIDE.md` - Gu√≠a completa de entrenamiento
- `../models/README.md` - Convenciones de naming de modelos
- `../config/` - Configuraciones de entrenamiento
